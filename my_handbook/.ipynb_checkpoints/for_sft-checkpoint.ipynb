{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, setup_chat_format\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, set_seed, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from accelerate import Accelerator\n",
    "from typing import Dict\n",
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset processing\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = [\"messages\", \"chosen\", \"rejected\", \"prompt\", \"completion\", \"label\", \"question\", \"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_datasets(dataset_mixer, shuffle=True, seed=42, test_percentage=0.1):\n",
    "    '''\n",
    "    Example format for dataset_nmixer:\n",
    "    dataset_mixer = {\n",
    "            \"dataset1\": 1, # dataset_name: proportion\n",
    "            # \"dataset1\": 0.3,\n",
    "            # \"dataset1\": 0.2,\n",
    "                }\n",
    "    '''\n",
    "    raw_train_datasets = []\n",
    "    raw_test_datasets = []\n",
    "    new_dataset = DatasetDict()\n",
    "    for key, value in dataset_mixer.items():\n",
    "        dataset = load_dataset(key)\n",
    "        if \"train\" in dataset:\n",
    "            train_dataset = dataset[\"train\"]\n",
    "            train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col not in COLUMNS_TO_KEEP])\n",
    "            raw_train_datasets.append((train_dataset, value))\n",
    "\n",
    "        if \"test\" in dataset:\n",
    "            test_dataset = dataset[\"test\"]\n",
    "            test_dataset = test_dataset.remove_columns([col for col in test_dataset.column_names if col not in COLUMNS_TO_KEEP])\n",
    "            raw_test_datasets.append(test_dataset)\n",
    "    train_subsets = []\n",
    "    for (dataset, frac) in raw_train_datasets:\n",
    "        train_subset = dataset.select(range(int(len(dataset)*frac)))\n",
    "        train_subsets.append(train_subset)\n",
    "    if shuffle:\n",
    "        train_dataset = concatenate_datasets(train_subsets).shuffle(seed=seed)\n",
    "    else:\n",
    "        train_dataset = concatenate_datasets(train_subsets)\n",
    "    if len(raw_test_datasets) > 0:\n",
    "        test_dataset = concatenate_datasets(raw_test_datasets).shuffle(seed=seed)\n",
    "    else:\n",
    "        test_dataset = None\n",
    "    \n",
    "    new_dataset['train'] = train_dataset\n",
    "\n",
    "    if test_dataset is None:\n",
    "        new_dataset = new_dataset['train'].train_test_split(test_size=test_percentage)\n",
    "    else:\n",
    "        new_dataset['test'] = test_dataset\n",
    "    \n",
    "    return new_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(example, tokenizer, task=\"sft\"):\n",
    "    '''\n",
    "    task can be: sft, rm, dpo, generation\n",
    "    '''\n",
    "    if task == \"sft\" or task == \"generation\":\n",
    "        if \"messages\" in example:\n",
    "            if example[\"messages\"][0][\"role\"] == \"system\":\n",
    "                messages = example[\"messages\"]\n",
    "            else:\n",
    "                messages = [{\"role\":\"system\", \"content\": \"\"}] + example[\"messages\"]\n",
    "        elif \"question\" in example and \"answer\" in example:\n",
    "            if \"system_prompt\" in example:\n",
    "                messages = [{\"role\":\"system\", \"content\": example['system_prompt']},{\"role\": \"user\", \"content\": example[\"question\"]}, {\"role\": \"assistant\", \"content\": example[\"answer\"]}]\n",
    "            else:\n",
    "                messages = [{\"role\":\"system\", \"content\": \"\"},{\"role\": \"user\", \"content\": example[\"question\"]}, {\"role\": \"assistant\", \"content\": example[\"answer\"]}]\n",
    "        example['text'] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True if task == \"generation\" else False)\n",
    "\n",
    "    elif task == \"rm\":\n",
    "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "            chosen_messages = example[\"chosen\"]\n",
    "            rejected_messages = example[\"rejected\"]\n",
    "            # We add an empty system message if there is none\n",
    "            if chosen_messages[0][\"role\"] != \"system\":\n",
    "                chosen_messages = [{\"role\":\"system\", \"content\": \"\"}] + chosen_messages\n",
    "            \n",
    "            if rejected_messages[0][\"role\"] != \"system\":\n",
    "                rejected_messages = [{\"role\":\"system\", \"content\": \"\"}] + rejected_messages\n",
    "\n",
    "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
    "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not format example as dialogue for `rm` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "            )\n",
    "    elif task == \"dpo\":\n",
    "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "\n",
    "            if isinstance(example['chosen'], list):\n",
    "                prompt_message = example['chosen'][:-1]\n",
    "                if prompt_message[0][\"role\"] != \"system\":\n",
    "                    prompt_message = [{\"role\":\"system\", \"content\": \"\"}] + prompt_message\n",
    "                chosen_message = example['chosen'][-1:]\n",
    "                rejected_message = example['rejected'][-1:]\n",
    "                example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_message, tokenize=False)\n",
    "                example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_message, tokenize=False)\n",
    "                example[\"text_prompt\"] = tokenizer.apply_chat_template(prompt_message, tokenize=False)\n",
    "            else:\n",
    "                example['text_chosen'] = tokenizer.apply_chat_template(example['chosen'], tokenize=False)\n",
    "                example['text_rejected'] = tokenizer.apply_chat_template(example['rejected'], tokenize=False)\n",
    "                example['text_prompt'] = tokenizer.apply_chat_template(example['prompt'], tokenize=False)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Task {task} not recognized\")\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Example###\n",
    "dataset_mixer = {\n",
    "            \"HuggingFaceH4/no_robots\": 1, # dataset_name: proportion\n",
    "            # \"dataset1\": 0.3,\n",
    "            # \"dataset1\": 0.2,\n",
    "                }\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME\n",
    "    )\n",
    "tokenizer.model_max_length = 2048\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "\n",
    "dataset = mix_datasets(dataset_mixer)\n",
    "\n",
    "dataset = dataset.map(\n",
    "        apply_template,\n",
    "        fn_kwargs={\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"task\": \"sft\",\n",
    "        },\n",
    "        num_proc=4,\n",
    "        remove_columns=list(dataset[\"train\"].features),\n",
    "        desc=\"Applying chat template\",\n",
    "    )\n",
    "num_raw_train_samples = len(dataset[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_4bit = True\n",
    "load_in_8bit = False\n",
    "quantization_config = None\n",
    "if load_in_4bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=\"auto\",\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "if load_in_8bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##detect device map\n",
    "def get_current_device() -> int:\n",
    "    \"\"\"Get the current device. For GPU we return the local process index to enable multiple GPU training.\"\"\"\n",
    "    return Accelerator().local_process_index if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_kbit_device_map() -> Dict[str, int] | None:\n",
    "    \"\"\"Useful for running inference with quantized models by setting `device_map=get_peft_device_map()`\"\"\"\n",
    "    return {\"\": get_current_device()} if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_peft = True\n",
    "\n",
    "if use_peft:\n",
    "    peft_config = LoraConfig(\n",
    "            r=64,\n",
    "            lora_alpha=128,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=['q_proj', 'k_proj', 'o_proj', \"v_proj\"],\n",
    "            modules_to_save=['q_proj', 'k_proj', 'o_proj', \"v_proj\"],\n",
    "        )\n",
    "else:\n",
    "    peft_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model kwargs\n",
    "\n",
    "model_kwargs = dict(\n",
    "        trust_remote_code=True,\n",
    "        use_flash_attention_2=False,\n",
    "        torch_dtype=\"auto\",\n",
    "        use_cache=False,\n",
    "        device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "model = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"<|im_start|>\" in tokenizer.chat_template:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model, **model_kwargs)\n",
    "    model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "    model_kwargs = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    dataloader_drop_last=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=5,\n",
    "    save_steps=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=1e-4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"thangvip/\"+output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=2048,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        peft_config=peft_config,\n",
    "        seq_length=2048,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        infinite=True,\n",
    "        # dataset_kwargs=dataset_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_last_checkpoint = False\n",
    "checkpoint_to_resume = None\n",
    "checkpoint = None\n",
    "last_checkpoint = get_last_checkpoint()\n",
    "if resume_from_last_checkpoint and last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "elif checkpoint_to_resume is not None:\n",
    "    checkpoint = checkpoint_to_resume\n",
    "\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "logger.info(\"*** Save model ***\")\n",
    "trainer.save_model(output_dir)\n",
    "logger.info(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "        \"finetuned_from\": MODEL_NAME,\n",
    "        \"dataset\": list(dataset_mixer.keys()),\n",
    "        \"dataset_tags\": list(dataset_mixer.keys()),\n",
    "    }\n",
    "\n",
    "if trainer.accelerator.is_main_process:\n",
    "    trainer.create_model_card(**kwargs)\n",
    "    # Restore k,v cache for fast inference\n",
    "    trainer.model.config.use_cache = True\n",
    "    trainer.model.config.save_pretrained(output_dir)\n",
    "\n",
    "do_eval = True\n",
    "\n",
    "if do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate()\n",
    "    metrics[\"eval_samples\"] = len(eval_dataset)\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "push_to_hub = True\n",
    "if push_to_hub is True:\n",
    "    logger.info(\"Pushing to hub...\")\n",
    "    trainer.push_to_hub(**kwargs)\n",
    "logger.info(\"*** Training complete ***\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
