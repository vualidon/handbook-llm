{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "from transformers import AutoModelForSeq2SeqLM, set_seed, AutoTokenizer, TrainingArguments\n",
    "from accelerate import Accelerator\n",
    "from typing import Dict\n",
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import logging\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset processing\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_datasets(dataset_mixer, shuffle=True, seed=42, test_percentage=0.1):\n",
    "    '''\n",
    "    Example format for dataset_nmixer:\n",
    "    dataset_mixer = {\n",
    "            \"dataset1\": 1, # dataset_name: proportion\n",
    "            # \"dataset1\": 0.3,\n",
    "            # \"dataset1\": 0.2,\n",
    "                }\n",
    "    '''\n",
    "    raw_train_datasets = []\n",
    "    raw_test_datasets = []\n",
    "    new_dataset = DatasetDict()\n",
    "    for key, value in dataset_mixer.items():\n",
    "        dataset = load_dataset(key)\n",
    "        if \"train\" in dataset:\n",
    "            train_dataset = dataset[\"train\"]\n",
    "            # train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col not in COLUMNS_TO_KEEP])\n",
    "            raw_train_datasets.append((train_dataset, value))\n",
    "\n",
    "        if \"test\" in dataset:\n",
    "            test_dataset = dataset[\"test\"]\n",
    "            # test_dataset = test_dataset.remove_columns([col for col in test_dataset.column_names if col not in COLUMNS_TO_KEEP])\n",
    "            raw_test_datasets.append(test_dataset)\n",
    "    train_subsets = []\n",
    "    for (dataset, frac) in raw_train_datasets:\n",
    "        train_subset = dataset.select(range(int(len(dataset)*frac)))\n",
    "        train_subsets.append(train_subset)\n",
    "    if shuffle:\n",
    "        train_dataset = concatenate_datasets(train_subsets).shuffle(seed=seed)\n",
    "    else:\n",
    "        train_dataset = concatenate_datasets(train_subsets)\n",
    "    if len(raw_test_datasets) > 0:\n",
    "        test_dataset = concatenate_datasets(raw_test_datasets).shuffle(seed=seed)\n",
    "    else:\n",
    "        test_dataset = None\n",
    "    \n",
    "    new_dataset['train'] = train_dataset\n",
    "\n",
    "    if test_dataset is None:\n",
    "        new_dataset = new_dataset['train'].train_test_split(test_size=test_percentage)\n",
    "    else:\n",
    "        new_dataset['test'] = test_dataset\n",
    "    \n",
    "    return new_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples['question']]\n",
    "    targets = [ex for ex in examples['queries']]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "    )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Example###\n",
    "dataset_mixer = {\n",
    "            \"HuggingFaceH4/no_robots\": 1, # dataset_name: proportion\n",
    "            # \"dataset1\": 0.3,\n",
    "            # \"dataset1\": 0.2,\n",
    "                }\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME\n",
    "    )\n",
    "# tokenizer.model_max_length = 2048\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "\n",
    "dataset = mix_datasets(dataset_mixer)\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "num_raw_train_samples = len(dataset[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model kwargs\n",
    "\n",
    "model_kwargs = dict(\n",
    "        trust_remote_code=True,\n",
    "        use_flash_attention_2=False,\n",
    "        torch_dtype=\"auto\",\n",
    "        use_cache=False,\n",
    "    )\n",
    "model = MODEL_NAME\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    dataloader_drop_last=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=5,\n",
    "    save_steps=100,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "    push_to_hub=True,\n",
    "    predict_with_generate=True,\n",
    "    save_total_limit=3,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    hub_model_id=\"thangvip/\"+output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=2048,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        seq_length=2048,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        infinite=True,\n",
    "        # dataset_kwargs=dataset_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "        \"finetuned_from\": MODEL_NAME,\n",
    "        \"dataset\": list(dataset_mixer.keys()),\n",
    "        \"dataset_tags\": list(dataset_mixer.keys()),\n",
    "    }\n",
    "\n",
    "if trainer.accelerator.is_main_process:\n",
    "    trainer.create_model_card(**kwargs)\n",
    "    # Restore k,v cache for fast inference\n",
    "    trainer.model.config.use_cache = True\n",
    "    trainer.model.config.save_pretrained(output_dir)\n",
    "\n",
    "do_eval = False\n",
    "\n",
    "if do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate()\n",
    "    metrics[\"eval_samples\"] = len(eval_dataset)\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "push_to_hub = True\n",
    "if push_to_hub is True:\n",
    "    logger.info(\"Pushing to hub...\")\n",
    "    trainer.push_to_hub(**kwargs)\n",
    "logger.info(\"*** Training complete ***\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
